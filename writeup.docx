1. I chose a ml.t3.medium instance size because all the work in the jupyter notebook that needed compute was not resource intensive. All the resource intensive work was done during the training, and inference. So choosing the cheapest option was the best route for me.

2. For the EC2 instance i chose t2.micro because i couldn't pick any other machines due to restrictions on AWS and needing to request a limit increase which you can't do. If able to choose an instance I would've used the g4dn.xlarge instance. The speed to price ratio makes it the most optimal for this training situation.

3. Since EC2 is less specialized compared to Sagemaker, a lot of the libraries are not included out of the box. All the Sagemaker libraries would need to be installed separately, unless the EC2 image has them pre installed (mine did not). In addition since we are calling the solution.py file directly we no longer need the `if __name__ == __main__` and no longer need to include the argparser to get the hyperparameters.

4. [-3.3532333374023438, -1.036774754524231, -2.599670886993408, -0.6815837025642395, -1.8271737098693848, -2.139362096786499, -0.8207728266716003, -0.7344204783439636, -4.222881317138672, -0.5331478714942932, 0.0798342376947403, -0.6244913339614868, -0.08444422483444214, 0.9692515134811401, -1.0739692449569702, -0.27424049377441406, -3.1741816997528076, -0.035448238253593445, -2.1658527851104736, 2.156782627105713, -1.8546031713485718, -0.9482290744781494, -2.8106956481933594, -2.457078218460083, 0.5397177934646606, -4.63071346282959, -0.5724292993545532, -3.766103506088257, -3.253441095352173, -0.1361905336380005, -0.9588345885276794, -1.481760859489441, -2.6786742210388184, -0.21982601284980774, -4.751748085021973, -3.54055118560791, -1.1736575365066528, -1.0698155164718628, 0.7760046124458313, -2.2487635612487793, -1.0300415754318237, -0.5046771764755249, 1.6530119180679321, -2.2629637718200684, 0.8783060908317566, -6.9178786277771, -1.205615758895874, 0.6838828325271606, -1.5829393863677979, -1.4838906526565552, -1.0281881093978882, -3.318854808807373, -3.740490436553955, -0.6603196263313293, -3.7017922401428223, -0.7496342658996582, -3.812567949295044, -3.199700355529785, -1.8464374542236328, -1.0127861499786377, -3.8189849853515625, -4.757951736450195, -4.360888481140137, -2.5495920181274414, -0.8232588171958923, -1.7187528610229492, -0.7627556324005127, -3.9884047508239746, -1.5340560674667358, -0.20151574909687042, 0.7496613264083862, -1.8098905086517334, -3.119877338409424, -3.29232120513916, -2.7099814414978027, -1.7167458534240723, -4.739052772521973, -1.392820954322815, -2.849161386489868, -0.9628125429153442, -0.6311649084091187, -3.9114139080047607, 0.7513200640678406, -0.3580024242401123, -4.433473587036133, -3.6107311248779297, -1.3329322338104248, -5.158458232879639, -0.995604395866394, -0.38258054852485657, -3.7699403762817383, -2.1584153175354004, -3.4994332790374756, -2.8910839557647705, -4.718567371368408, -2.1744375228881836, -2.6238832473754883, -1.2183316946029663, -4.923173904418945, -5.218628883361816, -3.2293777465820312, -0.9285445809364319, -1.3672209978103638, -3.0100326538085938, -3.2288577556610107, -3.0894439220428467, -1.3886380195617676, -1.0454225540161133, -1.754335880279541, 0.6076391339302063, 0.11220987141132355, -0.6774883270263672, -2.6830217838287354, -2.8486218452453613, -1.2698612213134766, -0.6386910080909729, -3.377875328063965, 0.3378351926803589, -2.945516586303711, 1.3051881790161133, -0.5455235242843628, -2.9161064624786377, -1.8669089078903198, -4.456328392028809, -3.697103261947632, -2.563908576965332, -1.4901785850524902, 0.5632570385932922, -3.5099527835845947, -2.6020565032958984, -2.1017377376556396, -1.0429083108901978, -3.3775086402893066]

5. The AmazonSageMakerFullAccess role is probably too broad, given the lambda function doesn't need to access all of Sagemaker's APIs. So that could be modified to only restrict it to the policy it explicitly needs.

6. I used provisioned concurrency for this lab because i want the resources for the lambda function allocated at all times. I provisioned the lambda function with 2 concurrent instances. For auto scaling I set the minimum count to 1 and max to 3 with the target, cool down, and warm up all set to 30 seconds. This will make it relatively responsive.
